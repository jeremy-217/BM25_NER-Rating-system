# ner.py
"""
ç°¡åŒ–ç‰ˆ NER åˆ†æå·¥å…· - æ•´åˆ Gemini API

åŠŸèƒ½ç‰¹è‰²ï¼š
- å…§å»º spaCy NER æœå‹™é€²è¡Œå¯¦é«”è­˜åˆ¥
- ä½¿ç”¨ Gemini API é€²è¡Œæ™ºèƒ½åˆ†æå’Œè™•ç†
- æ•´åˆ RAG æª¢ç´¢çµæœ
- ç§»é™¤è©•åˆ†æ©Ÿåˆ¶ï¼Œå°ˆæ³¨æ–¼å¯¦é«”è­˜åˆ¥å’Œæ™ºèƒ½åˆ†æ

ä½¿ç”¨æ–¹æ³•ï¼š
    uv run python ner.py

è¦æ±‚ï¼š
- spaCy å’Œ en_core_web_trf æ¨¡å‹å·²å®‰è£
- é…ç½®æ­£ç¢ºçš„ Gemini API é‡‘é‘°
- é…ç½®æ­£ç¢ºçš„å¤–éƒ¨è³‡æ–™ API è¨­å®š
"""

import requests
import json
import re
import unicodedata
from collections import Counter
from typing import Dict, List, Tuple, Optional
from html import unescape
import google.generativeai as genai

# æ•´åˆ NER æœå‹™ç›¸é—œå°å…¥
try:
    import spacy
    SPACY_AVAILABLE = True
    print("âœ… spaCy å¯ç”¨ï¼Œå°‡ä½¿ç”¨å…§å»º NER æœå‹™")
except ImportError:
    SPACY_AVAILABLE = False
    print("âš ï¸ spaCy ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨å¤–éƒ¨ NER API")

# --- å¾ config.py åŒ¯å…¥è¨­å®š ---
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from config import (
        NER_API_URL, 
        EXTERNAL_API_URL, 
        EXTERNAL_API_HEADERS,
        GOOGLE_API_KEY,
        STABLE_FLASH_MODEL,
        GENERATION_CONFIG,
        DATA_CLEANING_CONFIG
    )
    print("âœ… æˆåŠŸè¼‰å…¥ NER å’Œ Gemini API ç›¸é—œé…ç½®")
except ImportError as e:
    print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ° config.py æˆ–å¿…è¦çš„è¨­å®š: {e}")
    print("è«‹ç¢ºä¿ config.py ä¸­åŒ…å«æ‰€æœ‰å¿…è¦çš„é…ç½®ã€‚")
    exit()

# é…ç½® Gemini API
try:
    genai.configure(api_key=GOOGLE_API_KEY)
    print("âœ… Gemini API é‡‘é‘°é…ç½®æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ é…ç½® Gemini API é‡‘é‘°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    exit()

# åˆå§‹åŒ– Gemini æ¨¡å‹
model = genai.GenerativeModel(
    model_name=STABLE_FLASH_MODEL,
    generation_config=GENERATION_CONFIG
)
print(f"âœ… Gemini æ¨¡å‹ '{model.model_name}' å·²æº–å‚™å°±ç·’")

# --- å‡½å¼å®šç¾© ---

class LocalNERService:
    """å…§å»ºçš„ NER æœå‹™é¡"""
    
    def __init__(self):
        self.nlp = None
        self.model_loaded = False
        self.loaded_model_name = None
        
        if SPACY_AVAILABLE:
            # å˜—è©¦è¼‰å…¥ Transformer æ¨¡å‹
            model_options = ["en_core_web_trf", "en_core_web_sm"]
            
            print(f"ğŸ” å˜—è©¦è¼‰å…¥ spaCy æ¨¡å‹ï¼Œå€™é¸é …ç›®: {model_options}")
            
            for model_name in model_options:
                try:
                    print(f"   æ­£åœ¨å˜—è©¦è¼‰å…¥: {model_name}")
                    self.nlp = spacy.load(model_name)
                    self.model_loaded = True
                    self.loaded_model_name = model_name
                    print(f"âœ… spaCy NER æ¨¡å‹ '{model_name}' æˆåŠŸè¼‰å…¥ï¼")
                    
                    # æ¸¬è©¦æ¨¡å‹åŠŸèƒ½
                    test_doc = self.nlp("Apple Inc. is based in Cupertino.")
                    test_entities = [ent.text for ent in test_doc.ents]
                    print(f"   ğŸ§ª æ¨¡å‹æ¸¬è©¦: è­˜åˆ¥å‡º {len(test_entities)} å€‹å¯¦é«”: {test_entities}")
                    break
                except OSError as e:
                    print(f"   âŒ æ¨¡å‹ '{model_name}' è¼‰å…¥å¤±æ•—: {e}")
                    continue
                except Exception as e:
                    print(f"   ğŸ’¥ æ¨¡å‹ '{model_name}' æ¸¬è©¦å¤±æ•—: {e}")
                    continue
            
            if not self.model_loaded:
                print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ä»»ä½•å¯ç”¨çš„ spaCy æ¨¡å‹ã€‚")
                print("ğŸ’¡ å»ºè­°åŸ·è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£æ¨¡å‹ï¼š")
                print("   uv run python -m spacy download en_core_web_sm")
                print("   uv run python -m spacy download en_core_web_trf")
                print("ğŸ”„ å°‡å›é€€åˆ°å¤–éƒ¨ NER APIã€‚")
        else:
            print("âŒ spaCy ä¸å¯ç”¨ï¼Œå°‡åªä½¿ç”¨å¤–éƒ¨ NER API")
    
    def get_model_info(self):
        """è¿”å›ç•¶å‰è¼‰å…¥çš„æ¨¡å‹ä¿¡æ¯"""
        if self.model_loaded and self.nlp:
            return {
                "model_name": self.loaded_model_name,
                "model_loaded": True,
                "spacy_version": spacy.__version__,
                "model_lang": self.nlp.lang,
                "pipeline_components": list(self.nlp.pipe_names)
            }
        else:
            return {
                "model_name": None,
                "model_loaded": False,
                "spacy_version": spacy.__version__ if SPACY_AVAILABLE else "N/A",
                "error": "No model loaded"
            }
    
    def extract_entities(self, text: str) -> List[Dict]:
        """ä½¿ç”¨å…§å»º spaCy æ¨¡å‹æå–å¯¦é«”"""
        if not self.model_loaded or not self.nlp:
            return []
        
        try:
            doc = self.nlp(text)
            raw_entities = [
                {
                    "text": ent.text, 
                    "label": ent.label_,
                    "confidence": 1.0,  # spaCy ä¸æä¾›ä¿¡å¿ƒåº¦ï¼Œè¨­ç‚º 1.0
                    "start": ent.start_char,
                    "end": ent.end_char
                } 
                for ent in doc.ents
            ]
            
            # å¾Œè™•ç†ï¼šéæ¿¾å’Œä¿®æ­£å¯¦é«”
            filtered_entities = self._post_process_entities(raw_entities, text)
            return filtered_entities
            
        except Exception as e:
            print(f"âš ï¸ å…§å»º NER æœå‹™éŒ¯èª¤: {e}")
            return []
    
    def _post_process_entities(self, entities: List[Dict], text: str) -> List[Dict]:
        """å¾Œè™•ç†å¯¦é«”ï¼Œéæ¿¾èª¤è­˜åˆ¥å’Œä¿®æ­£é‚Šç•Œ"""
        filtered_entities = []
        
        # å®šç¾©éæ¿¾è¦å‰‡
        exclude_patterns = {
            'PERSON': [r'^Fig$', r'^mA$', r'^Table$', r'^\d+$'],  # æ’é™¤åœ–è¡¨æ¨™ç±¤è¢«èª¤è­˜ç‚ºäººå
            'ORG': [r'^Table\s+\d+$', r'^Fig\s+\d+$'],  # æ’é™¤è¡¨æ ¼/åœ–ç‰‡æ¨™ç±¤è¢«èª¤è­˜ç‚ºçµ„ç¹”
            'GPE': [r'^\d+$', r'^[A-Z]{1,3}$']  # æ’é™¤å–®å€‹å­—æ¯æˆ–æ•¸å­—è¢«èª¤è­˜ç‚ºåœ°å
        }
        
        # å®šç¾©æœ€å°é•·åº¦è¦æ±‚
        min_lengths = {
            'PERSON': 2,
            'ORG': 2,
            'GPE': 2
        }
        
        for entity in entities:
            entity_text = entity['text'].strip()
            entity_label = entity['label']
            
            # è·³éç©ºå¯¦é«”
            if not entity_text:
                continue
            
            # æª¢æŸ¥æœ€å°é•·åº¦
            if entity_label in min_lengths and len(entity_text) < min_lengths[entity_label]:
                continue
            
            # æª¢æŸ¥æ’é™¤æ¨¡å¼
            should_exclude = False
            if entity_label in exclude_patterns:
                for pattern in exclude_patterns[entity_label]:
                    if re.match(pattern, entity_text):
                        should_exclude = True
                        break
            
            if should_exclude:
                continue
            
            # æ¸…ç†å¯¦é«”æ–‡æœ¬
            cleaned_text = self._clean_entity_text(entity_text)
            if cleaned_text:
                entity['text'] = cleaned_text
                filtered_entities.append(entity)
        
        return filtered_entities
    
    def _clean_entity_text(self, text: str) -> str:
        """æ¸…ç†å¯¦é«”æ–‡æœ¬"""
        # ç§»é™¤å‰å¾Œæ¨™é»ç¬¦è™Ÿ
        text = re.sub(r'^[^\w]+|[^\w]+$', '', text)
        
        # ç§»é™¤å¤šé¤˜ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text if len(text) >= 2 else ""
    
    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """å»é™¤é‡è¤‡å¯¦é«”ï¼Œä¿ç•™æœ€é•·ç‰ˆæœ¬"""
        if not entities:
            return entities
        
        # æŒ‰é¡å‹åˆ†çµ„
        entities_by_type = {}
        for entity in entities:
            label = entity['label']
            if label not in entities_by_type:
                entities_by_type[label] = []
            entities_by_type[label].append(entity)
        
        deduplicated = []
        for label, type_entities in entities_by_type.items():
            # æŒ‰æ–‡æœ¬å…§å®¹å»é‡ï¼Œä¿ç•™æœ€é•·çš„ç‰ˆæœ¬
            unique_entities = {}
            for entity in type_entities:
                text = entity['text'].lower()
                if text not in unique_entities or len(entity['text']) > len(unique_entities[text]['text']):
                    unique_entities[text] = entity
            
            deduplicated.extend(unique_entities.values())
        
        return deduplicated

# å…¨åŸŸ NER æœå‹™å¯¦ä¾‹
local_ner_service = LocalNERService()

def fetch_chunks_from_api(query: str) -> list:
    """
    å‘å¤–éƒ¨ API ç™¼é€è«‹æ±‚ä»¥ç²å–èˆ‡æŸ¥è©¢ç›¸é—œçš„æ–‡ä»¶ chunksã€‚
    """
    print(f"\næ­£åœ¨å‘å¤–éƒ¨è³‡æ–™ API æŸ¥è©¢: '{query}'")
    print(f"API ç«¯é»: {EXTERNAL_API_URL}")
    
    data_to_send = {"query": query}
    
    try:
        print(f"ğŸ“¤ ç™¼é€è«‹æ±‚æ•¸æ“š: {data_to_send}")
        print(f"ğŸ“¤ è«‹æ±‚æ¨™é ­: {EXTERNAL_API_HEADERS}")
        
        response = requests.post(EXTERNAL_API_URL, headers=EXTERNAL_API_HEADERS, json=data_to_send, timeout=30)
        
        print(f"ğŸ“¥ å›æ‡‰ç‹€æ…‹ç¢¼: {response.status_code}")
        print(f"ğŸ“¥ å›æ‡‰æ¨™é ­: {dict(response.headers)}")
        
        if response.status_code != 200:
            print(f"âŒ API éŒ¯èª¤è©³æƒ…: {response.text}")
            print(f"âŒ è«‹æ±‚ URL: {response.url}")
            
            # æä¾›æ¸¬è©¦æ•¸æ“šä½œç‚ºå›é€€
            print("ğŸ”„ ä½¿ç”¨æ¸¬è©¦æ•¸æ“šä½œç‚ºå›é€€...")
            return [
                {
                    "id": "test_fallback_001",
                    "content": f"This is a test document about {query}. Apple Inc. is a major technology company founded by Steve Jobs in Cupertino, California. The company is known for innovative products like iPhone, iPad, and Mac computers."
                },
                {
                    "id": "test_fallback_002", 
                    "content": f"Another document discussing {query}. TSMC (Taiwan Semiconductor Manufacturing Company) is the world's largest contract chip manufacturer, producing semiconductors for companies like Apple, Nvidia, AMD, and Qualcomm."
                }
            ]
        
        response.raise_for_status()
        print("âœ… æˆåŠŸå¾è³‡æ–™ API ç²å– chunkï¼")
        
        # å‡è¨­ API å›æ‡‰çš„æ ¼å¼æ˜¯ JSONï¼Œä¸” chunks å°±åœ¨å…¶ä¸­
        api_response = response.json()
        print(f"ğŸ“‹ API å›æ‡‰é¡å‹: {type(api_response)}")
        
        # è™•ç† API å¯èƒ½è¿”å›å–®ä¸€ç‰©ä»¶æˆ–åˆ—è¡¨çš„æƒ…æ³
        if isinstance(api_response, list):
            print(f"ğŸ“‹ è¿”å›åˆ—è¡¨ï¼ŒåŒ…å« {len(api_response)} å€‹é …ç›®")
            return api_response
        elif isinstance(api_response, dict):
            print(f"ğŸ“‹ è¿”å›å­—å…¸ï¼Œéµ: {list(api_response.keys())}")
            # å¦‚æœ API è¿”å›çš„æ˜¯ä¸€å€‹å­—å…¸ï¼Œæˆ‘å€‘å‡è¨­ chunks åœ¨æŸå€‹éµä¸‹ï¼Œæˆ–å®ƒæœ¬èº«å°±æ˜¯ä¸€å€‹ chunk
            # é€™è£¡æˆ‘å€‘ç°¡å–®åœ°å°‡å…¶æ”¾å…¥åˆ—è¡¨ä¸­ä»¥çµ±ä¸€è™•ç†ï¼Œæ‚¨å¯ä»¥æ ¹æ“šå¯¦éš›æƒ…æ³èª¿æ•´
            return [api_response]
        else:
            print(f"âš ï¸ è­¦å‘Š: å¾ API æ”¶åˆ°äº†æœªçŸ¥çš„è³‡æ–™æ ¼å¼: {type(api_response)}")
            return []
            
    except requests.exceptions.Timeout:
        print(f"â° éŒ¯èª¤: API è«‹æ±‚è¶…æ™‚")
        return None
    except requests.exceptions.ConnectionError:
        print(f"ğŸ”Œ éŒ¯èª¤: ç„¡æ³•é€£æ¥åˆ° API ä¼ºæœå™¨")
        return None
    except requests.exceptions.RequestException as e:
        print(f"âŒ éŒ¯èª¤: å¤–éƒ¨è³‡æ–™ API è«‹æ±‚å¤±æ•—: {e}")
        print(f"   è«‹æ±‚ URL: {EXTERNAL_API_URL}")
        print(f"   è«‹æ±‚æ•¸æ“š: {data_to_send}")
        print(f"   è«‹æ±‚æ¨™é ­: {EXTERNAL_API_HEADERS}")
        return None
    except Exception as e:
        print(f"ğŸ’¥ æœªé æœŸçš„éŒ¯èª¤: {e}")
        return None


def clean_text_for_ner(text: str) -> str:
    """
    ç‚º NER åˆ†ææ¸…ç†æ–‡æœ¬æ•¸æ“š
    """
    if not text or not text.strip():
        return ""
    
    original_text = text
    cleaning_stats = {
        "original_length": len(text),
        "steps_applied": []
    }
    
    # 1. ç§»é™¤ HTML æ¨™ç±¤
    if DATA_CLEANING_CONFIG.get("remove_html_tags", True):
        text = re.sub(r'<[^>]+>', '', text)
        text = unescape(text)  # è§£ç¢¼ HTML å¯¦é«”
        cleaning_stats["steps_applied"].append("HTMLæ¨™ç±¤ç§»é™¤")
    
    # 2. ç§»é™¤ç‰¹å®šæ¨¡å¼ï¼ˆå¦‚åœ–ç‰‡æ¨™ç±¤ï¼‰
    for pattern in DATA_CLEANING_CONFIG.get("remove_patterns", []):
        old_len = len(text)
        text = re.sub(pattern, ' ', text, flags=re.IGNORECASE | re.DOTALL)
        if len(text) < old_len:
            cleaning_stats["steps_applied"].append(f"æ¨¡å¼ç§»é™¤: {pattern[:20]}...")
    
    # 3. ç§»é™¤ URL
    if DATA_CLEANING_CONFIG.get("remove_urls", True):
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)
        text = re.sub(r'www\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)
        cleaning_stats["steps_applied"].append("URLç§»é™¤")
    
    # 4. ç§»é™¤ email åœ°å€
    if DATA_CLEANING_CONFIG.get("remove_emails", True):
        text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', ' ', text)
        cleaning_stats["steps_applied"].append("Emailç§»é™¤")
    
    # 5. Unicode æ­£è¦åŒ–
    if DATA_CLEANING_CONFIG.get("normalize_unicode", True):
        text = unicodedata.normalize('NFKC', text)
        cleaning_stats["steps_applied"].append("Unicodeæ­£è¦åŒ–")
    
    # 6. ç§»é™¤å¤šé¤˜çš„ç©ºç™½å­—ç¬¦
    if DATA_CLEANING_CONFIG.get("remove_extra_whitespace", True):
        # ä¿ç•™å–®å€‹ç©ºæ ¼ï¼Œç§»é™¤å¤šé¤˜ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        cleaning_stats["steps_applied"].append("ç©ºç™½å­—ç¬¦æ¸…ç†")
    
    # 7. ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆå¯é¸ï¼Œå¯èƒ½å½±éŸ¿ç§‘å­¸ç¬¦è™Ÿï¼‰
    if DATA_CLEANING_CONFIG.get("remove_special_chars", False):
        # ä¿ç•™åŸºæœ¬æ¨™é»å’Œå­—æ¯æ•¸å­—
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)\[\]\{\}\"\'Â°%\/]', ' ', text)
        cleaning_stats["steps_applied"].append("ç‰¹æ®Šå­—ç¬¦ç§»é™¤")
    
    # 8. æª¢æŸ¥æ–‡æœ¬é•·åº¦é™åˆ¶
    min_length = DATA_CLEANING_CONFIG.get("min_text_length", 10)
    max_length = DATA_CLEANING_CONFIG.get("max_text_length", 10000)
    
    if len(text) < min_length:
        cleaning_stats["steps_applied"].append(f"æ–‡æœ¬éçŸ­ ({len(text)} < {min_length})")
        return ""
    
    if len(text) > max_length:
        text = text[:max_length]
        cleaning_stats["steps_applied"].append(f"æ–‡æœ¬æˆªæ–· (>{max_length})")
    
    # æœ€çµ‚æ¸…ç†
    text = text.strip()
    
    cleaning_stats["final_length"] = len(text)
    cleaning_stats["reduction_ratio"] = 1 - (len(text) / len(original_text)) if len(original_text) > 0 else 0
    
    return text


def get_entities_from_ner_service(text: str, enable_cleaning: bool = True, prefer_local: bool = True) -> Dict:
    """
    å‘ NER æœå‹™ç™¼é€è«‹æ±‚ï¼Œå„ªå…ˆä½¿ç”¨å…§å»ºæœå‹™ï¼ŒåŒ…å«æ•¸æ“šæ¸…ç†åŠŸèƒ½
    
    Args:
        text: è¦åˆ†æçš„æ–‡æœ¬
        enable_cleaning: æ˜¯å¦å•Ÿç”¨æ•¸æ“šæ¸…ç†
        prefer_local: æ˜¯å¦å„ªå…ˆä½¿ç”¨å…§å»º NER æœå‹™
    
    Returns:
        åŒ…å«å¯¦é«”å’Œæ¸…ç†çµ±è¨ˆçš„å­—å…¸
    """
    if not text or not text.strip():
        return {
            "entities": [],
            "cleaning_applied": False,
            "service_used": "none"
        }

    original_text = text
    cleaning_applied = False
    
    # æ•¸æ“šæ¸…ç†
    if enable_cleaning:
        cleaned_text = clean_text_for_ner(text)
        if cleaned_text != text:
            cleaning_applied = True
            text = cleaned_text
        
        if not text:  # æ¸…ç†å¾Œæ–‡æœ¬ç‚ºç©º
            return {
                "entities": [],
                "cleaning_applied": cleaning_applied,
                "service_used": "none"
            }
    
    # å˜—è©¦ä½¿ç”¨å…§å»º NER æœå‹™
    entities = []
    service_used = "external_api"
    
    if prefer_local and local_ner_service.model_loaded:
        try:
            entities = local_ner_service.extract_entities(text)
            service_used = "local_spacy"
            print(f"ğŸ§  ä½¿ç”¨å…§å»º spaCy NER æœå‹™")
        except Exception as e:
            print(f"âš ï¸ å…§å»º NER æœå‹™å¤±æ•—: {e}")
            entities = []
    
    # å¦‚æœå…§å»ºæœå‹™å¤±æ•—æˆ–ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤–éƒ¨ API
    if not entities and not prefer_local or (prefer_local and not entities):
        try:
            print(f"ğŸŒ ä½¿ç”¨å¤–éƒ¨ NER API: {NER_API_URL}")
            response = requests.post(NER_API_URL, json={"text": text}, timeout=10)
            response.raise_for_status()
            api_entities = response.json().get("entities", [])
            
            # è½‰æ›å¤–éƒ¨ API æ ¼å¼ç‚ºçµ±ä¸€æ ¼å¼
            entities = []
            for entity in api_entities:
                if isinstance(entity, dict):
                    entities.append({
                        "text": entity.get("text", ""),
                        "label": entity.get("label", "UNKNOWN"),
                        "confidence": entity.get("confidence", 1.0)
                    })
            
            service_used = "external_api"
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ å¤–éƒ¨ NER API è«‹æ±‚å¤±æ•—: {e}")
            entities = [{"text": f"NER Service Error: {str(e)}", "label": "ERROR", "confidence": 0.0}]
            service_used = "error"
    
    return {
        "entities": entities,
        "cleaning_applied": cleaning_applied,
        "original_length": len(original_text),
        "cleaned_length": len(text),
        "service_used": service_used
    }


def analyze_entities_with_gemini(entities: List[Dict], text: str, query: str) -> Dict:
    """
    ä½¿ç”¨ Gemini API åˆ†æå¯¦é«”è­˜åˆ¥çµæœ
    """
    if not entities:
        return {
            "entity_count": 0,
            "gemini_analysis": "æœªè­˜åˆ¥åˆ°ä»»ä½•å¯¦é«”",
            "entity_summary": {}
        }
    
    # æº–å‚™å¯¦é«”è³‡è¨Š
    entity_info = []
    entity_types = {}
    
    for entity in entities:
        entity_text = entity.get('text', '')
        entity_label = entity.get('label', 'UNKNOWN')
        confidence = entity.get('confidence', 1.0)
        
        entity_info.append(f"- {entity_text} ({entity_label}, ä¿¡å¿ƒåº¦: {confidence:.2f})")
        
        if entity_label not in entity_types:
            entity_types[entity_label] = 0
        entity_types[entity_label] += 1
    
    # æ§‹å»º Gemini åˆ†ææç¤º
    prompt = f"""
è«‹åˆ†æä»¥ä¸‹ NER (å‘½åå¯¦é«”è­˜åˆ¥) çµæœï¼š

åŸå§‹æŸ¥è©¢: {query}
æ–‡æœ¬é•·åº¦: {len(text)} å­—ç¬¦
è­˜åˆ¥åˆ°çš„å¯¦é«”æ•¸é‡: {len(entities)}

å¯¦é«”åˆ—è¡¨:
{chr(10).join(entity_info)}

å¯¦é«”é¡å‹çµ±è¨ˆ:
{chr(10).join([f"- {label}: {count}å€‹" for label, count in entity_types.items()])}

è«‹æä¾›ä»¥ä¸‹åˆ†æï¼š
1. å¯¦é«”è­˜åˆ¥çš„æ•´é«”å“è³ªè©•ä¼°
2. å¯¦é«”èˆ‡æŸ¥è©¢çš„ç›¸é—œæ€§
3. å¯¦é«”çš„å¯¦ç”¨æ€§å’Œé‡è¦æ€§
4. æ˜¯å¦æœ‰éºæ¼çš„é‡è¦å¯¦é«”
5. å°å¯¦é«”çµæœçš„ç¸½çµå’Œå»ºè­°

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œæä¾›è©³ç´°ä½†ç°¡æ½”çš„åˆ†æã€‚
"""
    
    try:
        response = model.generate_content(prompt)
        gemini_analysis = response.text
    except Exception as e:
        gemini_analysis = f"Gemini åˆ†æå¤±æ•—: {str(e)}"
    
    return {
        "entity_count": len(entities),
        "entity_types": entity_types,
        "gemini_analysis": gemini_analysis,
        "entity_summary": {
            "total_entities": len(entities),
            "unique_types": len(entity_types),
            "most_common_type": max(entity_types, key=entity_types.get) if entity_types else "ç„¡"
        }
    }


def display_ner_analysis(chunk_id: str, content: str, ner_result: Dict, analysis: Dict, query: str):
    """
    é¡¯ç¤º NER åˆ†æçµæœï¼ŒåŒ…å« Gemini æ™ºèƒ½åˆ†æ
    """
    entities = ner_result.get("entities", [])
    
    print(f"\n{'='*60}")
    print(f"ğŸ“„ æ–‡ä»¶ ID: {chunk_id}")
    print(f"ğŸ“ å…§å®¹é•·åº¦: {len(content)} å­—ç¬¦")
    print(f"ğŸ¯ å…§å®¹é è¦½: \"{content[:100]}...\"")
    print(f"ğŸ”§ NER æœå‹™: {ner_result.get('service_used', 'unknown')}")
    print(f"ğŸ“‹ æŸ¥è©¢å•é¡Œ: {query}")
    print(f"{'='*60}")
    
    # é¡¯ç¤ºæ•¸æ“šæ¸…ç†ä¿¡æ¯
    if ner_result.get("cleaning_applied", False):
        print(f"ğŸ§¹ æ•¸æ“šæ¸…ç†:")
        print(f"   ğŸ“ åŸå§‹é•·åº¦: {ner_result.get('original_length', 0)} å­—ç¬¦")
        print(f"   âœ‚ï¸  æ¸…ç†å¾Œé•·åº¦: {ner_result.get('cleaned_length', 0)} å­—ç¬¦")
        reduction = ner_result.get('original_length', 0) - ner_result.get('cleaned_length', 0)
        if reduction > 0:
            reduction_pct = (reduction / ner_result.get('original_length', 1)) * 100
            print(f"   ğŸ“‰ ç§»é™¤å…§å®¹: {reduction} å­—ç¬¦ ({reduction_pct:.1f}%)")
    else:
        print(f"ğŸ§¹ æ•¸æ“šæ¸…ç†: æœªå•Ÿç”¨")
    
    # é¡¯ç¤ºå¯¦é«”çµ±è¨ˆ
    print(f"\nğŸ“Š NER è­˜åˆ¥çµæœ:")
    print(f"   ğŸ”¢ å¯¦é«”ç¸½æ•¸: {analysis['entity_count']}")
    
    if analysis['entity_types']:
        print(f"    å¯¦é«”é¡å‹åˆ†å¸ƒ:")
        sorted_types = sorted(analysis['entity_types'].items(), key=lambda x: x[1], reverse=True)
        for entity_type, count in sorted_types:
            percentage = (count / analysis['entity_count']) * 100
            print(f"      {entity_type}: {count} ({percentage:.1f}%)")
    
    # è©³ç´°å¯¦é«”åˆ—è¡¨
    print(f"\nğŸ” è­˜åˆ¥å‡ºçš„å¯¦é«”:")
    if not entities:
        print("   (ç„¡)")
    else:
        # æŒ‰é¡å‹åˆ†çµ„é¡¯ç¤º
        entities_by_type = {}
        for entity in entities:
            entity_type = entity.get('label', 'UNKNOWN')
            if entity_type not in entities_by_type:
                entities_by_type[entity_type] = []
            entities_by_type[entity_type].append(entity)
        
        for entity_type, type_entities in entities_by_type.items():
            print(f"\n   ğŸ“Œ {entity_type}:")
            for entity in type_entities:
                entity_text = entity.get('text', '')
                confidence = entity.get('confidence', 'N/A')
                if isinstance(confidence, (int, float)):
                    confidence_str = f"{confidence:.2f}"
                else:
                    confidence_str = str(confidence)
                print(f"      â€¢ {entity_text} (ä¿¡å¿ƒåº¦: {confidence_str})")
    
    # é¡¯ç¤º Gemini æ™ºèƒ½åˆ†æ
    print(f"\nğŸ¤– Gemini æ™ºèƒ½åˆ†æ:")
    print(f"{analysis['gemini_analysis']}")
    
    print(f"{'='*60}")


def analyze_rag_results_with_gemini(query: str, all_results: List[Dict]) -> str:
    """
    ä½¿ç”¨ Gemini API åˆ†ææ•´é«” RAG çµæœ
    """
    # æº–å‚™ RAG çµæœæ‘˜è¦
    total_chunks = len(all_results)
    total_entities = sum(result['analysis']['entity_count'] for result in all_results)
    
    # æ”¶é›†æ‰€æœ‰å¯¦é«”é¡å‹
    all_entity_types = {}
    chunk_summaries = []
    
    for result in all_results:
        chunk_id = result['id']
        entity_count = result['analysis']['entity_count']
        entity_types = result['analysis']['entity_types']
        
        chunk_summaries.append(f"æ–‡ä»¶ {chunk_id}: {entity_count}å€‹å¯¦é«”")
        
        for entity_type, count in entity_types.items():
            if entity_type not in all_entity_types:
                all_entity_types[entity_type] = 0
            all_entity_types[entity_type] += count
    
    # æ§‹å»º Gemini åˆ†ææç¤º
    prompt = f"""
è«‹åˆ†æä»¥ä¸‹ RAG (æª¢ç´¢å¢å¼·ç”Ÿæˆ) ç³»çµ±çš„æ•´é«”è¡¨ç¾ï¼š

ä½¿ç”¨è€…æŸ¥è©¢: {query}

æª¢ç´¢çµæœçµ±è¨ˆ:
- æª¢ç´¢åˆ°çš„æ–‡ä»¶æ•¸é‡: {total_chunks}
- ç¸½è­˜åˆ¥å¯¦é«”æ•¸é‡: {total_entities}
- å¹³å‡æ¯æ–‡ä»¶å¯¦é«”æ•¸: {total_entities/max(total_chunks, 1):.1f}

å„æ–‡ä»¶å¯¦é«”çµ±è¨ˆ:
{chr(10).join(chunk_summaries)}

æ•´é«”å¯¦é«”é¡å‹åˆ†å¸ƒ:
{chr(10).join([f"- {label}: {count}å€‹" for label, count in sorted(all_entity_types.items(), key=lambda x: x[1], reverse=True)])}

è«‹æä¾›ä»¥ä¸‹åˆ†æï¼š
1. RAG æª¢ç´¢çµæœçš„å“è³ªè©•ä¼°
2. å¯¦é«”è­˜åˆ¥å°å›ç­”ä½¿ç”¨è€…æŸ¥è©¢çš„å¹«åŠ©ç¨‹åº¦
3. æª¢ç´¢åˆ°çš„æ–‡ä»¶æ˜¯å¦èƒ½å¤ å……åˆ†å›ç­”ä½¿ç”¨è€…å•é¡Œ
4. å¯¦é«”åˆ†å¸ƒæ˜¯å¦ç¬¦åˆæŸ¥è©¢ä¸»é¡Œ
5. æ•´é«”ç³»çµ±è¡¨ç¾çš„æ”¹é€²å»ºè­°

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œæä¾›è©³ç´°çš„åˆ†æå’Œå»ºè­°ã€‚
"""
    
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Gemini æ•´é«”åˆ†æå¤±æ•—: {str(e)}"


def generate_summary_report(all_results: List[Dict], query: str):
    """
    ç”Ÿæˆæ•´é«”çš„ NER å’Œ RAG åˆ†ææ‘˜è¦å ±å‘Š
    """
    print(f"\n{'='*80}")
    print(f"æ•´é«” NER + RAG åˆ†ææ‘˜è¦å ±å‘Š")
    print(f"{'='*80}")
    
    total_chunks = len(all_results)
    total_entities = sum(result['analysis']['entity_count'] for result in all_results)
    
    print(f"åŸºæœ¬çµ±è¨ˆ:")
    print(f" æŸ¥è©¢å•é¡Œ: {query}")
    print(f" æ–‡ä»¶å¡Šç¸½æ•¸: {total_chunks}")
    print(f" å¯¦é«”ç¸½æ•¸: {total_entities}")
    print(f" å¹³å‡æ¯æ–‡ä»¶å¯¦é«”æ•¸: {total_entities/max(total_chunks, 1):.1f}")
    
    # ç²å– Gemini æ•´é«”åˆ†æ
    print(f"\nğŸ¤– Gemini æ•´é«” RAG åˆ†æ:")
    rag_analysis = analyze_rag_results_with_gemini(query, all_results)
    print(rag_analysis)
    
    print(f"{'='*80}")


# --- ä¸»ç¨‹å¼åŸ·è¡Œå€å¡Š ---

if __name__ == "__main__":
    # --- æ­¥é©Ÿ 1: æ¥æ”¶ä½¿ç”¨è€…è¼¸å…¥ ---
    user_query = input("è«‹è¼¸å…¥æ‚¨çš„æŸ¥è©¢å•é¡Œä»¥ç²å–ç›¸é—œæ–‡ä»¶ chunks: ")
    if not user_query:
        print("æŸ¥è©¢ä¸èƒ½ç‚ºç©ºã€‚")
        exit()

    print("\n" + "="*80)
    print("å¾ API æª¢ç´¢æ–‡ä»¶ä¸¦é€²è¡Œ NER + Gemini æ™ºèƒ½åˆ†æ")
    print("="*80)

    # --- æ­¥é©Ÿ 2: å¾æ‚¨çš„ API ç²å–æ–‡ä»¶ chunks ---
    retrieved_chunks = fetch_chunks_from_api(user_query)

    if retrieved_chunks is None:
        print("âŒ ç„¡æ³•ç²å–è³‡æ–™ï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
        print("ğŸ’¡ å»ºè­°åŸ·è¡Œæ¸¬è©¦: uv run python ner_test_suite.py")
        exit()
    
    if not retrieved_chunks:
        print("âš ï¸ API æœªè¿”å›ä»»ä½•æ–‡ä»¶ chunkã€‚")
        exit()

    print(f"\nâœ… å…±æª¢ç´¢åˆ° {len(retrieved_chunks)} ä»½æ–‡ä»¶ chunkï¼Œç¾é€ä¸€é€²è¡Œ NER + Gemini åˆ†æ...")

    # --- æ­¥é©Ÿ 3: å°æ¯å€‹ chunk é€²è¡Œ NER ä¸¦ä½¿ç”¨ Gemini åˆ†æ ---
    all_results = []
    for i, chunk in enumerate(retrieved_chunks):
        # å‡è¨­ chunk æ˜¯å­—å…¸ï¼Œä¸”å…§å®¹åœ¨ 'content' éµä¸‹
        content_to_analyze = chunk.get("content", "")
        chunk_id = chunk.get("id", f"unknown_{i+1}")

        # å‘¼å« NER æœå‹™
        extracted_entities = get_entities_from_ner_service(content_to_analyze)
        
        # ä½¿ç”¨ Gemini åˆ†æå¯¦é«”çµæœ
        analysis = analyze_entities_with_gemini(
            extracted_entities.get('entities', []), 
            content_to_analyze, 
            user_query
        )
        
        # å„²å­˜çµæœ
        result = {
            "id": chunk_id,
            "entities": extracted_entities,
            "analysis": analysis,
            "content_length": len(content_to_analyze)
        }
        all_results.append(result)

        # é¡¯ç¤ºè©³ç´°åˆ†æçµæœ
        display_ner_analysis(chunk_id, content_to_analyze, extracted_entities, analysis, user_query)

    # --- æ­¥é©Ÿ 4: ç”Ÿæˆæ•´é«”æ‘˜è¦å ±å‘Šï¼ˆåŒ…å« Gemini RAG åˆ†æï¼‰ ---
    generate_summary_report(all_results, user_query)
    
    # --- æ­¥é©Ÿ 5: å¯é¸æ“‡æ€§åœ°å°‡çµæœå­˜å„²åˆ°æª”æ¡ˆä¸­ ---
    save_results = input("\næ˜¯å¦è¦å°‡åˆ†æçµæœä¿å­˜åˆ° JSON æª”æ¡ˆï¼Ÿ (y/N): ").lower().strip()
    if save_results in ['y', 'yes', 'æ˜¯']:
        filename = "ner_gemini_analysis_results.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        print(f"âœ… åˆ†æçµæœå·²å„²å­˜åˆ° {filename}")
    
    print("\nğŸ‰ NER + Gemini æ™ºèƒ½åˆ†æå®Œæˆï¼")